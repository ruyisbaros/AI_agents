{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "client = Groq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In the context of a conversational AI model like me, `temperature` is a parameter that controls the randomness and creativity of the generated responses.\n",
       "\n",
       "**Temperature (τ)** is a value between 0 and 1 that determines how \"cautious\" or \"adventurous\" the model is when generating text. Here's a rough interpretation of different temperature values:\n",
       "\n",
       "* **Temperature = 0**: The model will generate the most likely response, based on the patterns it has learned from the training data. This will result in a very predictable and \"safe\" answer.\n",
       "* **Temperature = 1**: The model will generate a more random and creative response, as it will explore more possibilities and take more risks. This can lead to more interesting and varied answers, but also increases the chance of errors or nonsensical responses.\n",
       "* **Temperature between 0 and 1**: This is where things get interesting. A temperature value between 0 and 1 will balance the trade-off between predictability and creativity. A lower temperature (e.g., 0.3) will produce more conservative responses, while a higher temperature (e.g., 0.8) will produce more innovative ones.\n",
       "\n",
       "In your case, setting `temperature=0.7` suggests that you want the model to generate responses that are somewhat creative and varied, but still generally reliable and accurate.\n",
       "\n",
       "To give you a better sense of the difference, here are some possible characteristics of responses generated at different temperature values:\n",
       "\n",
       "* `Temperature=0.7` (your setting): Responses are likely to be informative, relevant, and engaging, with some level of creativity and variation.\n",
       "* `Temperature=1.0`: Responses may be more innovative, humorous, or thought-provoking, but also riskier and potentially less accurate.\n",
       "\n",
       "Keep in mind that the ideal temperature value depends on the specific task, the model's quality, and your personal preferences. Experimenting with different temperature values can help you find the sweet spot for your needs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\":\"user\", \n",
    "         \"content\":\"When I ask you question, I set temprature=0.7. But honestly I have no idea what is difference between temprature=0.7 and temprature=1.0. Can you explain it to me?\"}\n",
    "    ],\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    temperature=0.7\n",
    ")\n",
    "response = completion.choices[0].message.content\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, system, client=client):\n",
    "        self.system = system\n",
    "        self.client = client\n",
    "        self.messages = []\n",
    "\n",
    "        if self.system is not None:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": self.system})\n",
    "    \n",
    "    def __call__(self, message):\n",
    "        if message:\n",
    "            self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "        result = self.execute()\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\":result})\n",
    "        return result\n",
    "    \n",
    "    def execute(self):\n",
    "        completion = self.client.chat.completions.create(\n",
    "            messages=self.messages,\n",
    "            model=\"llama-3.1-70b-versatile\",\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You run in a loop of Thought, Action, PAUSE, Observation.\n",
    "At the end of the loop you output an Answer\n",
    "Use Thought to describe your thoughts about the question you have been asked.\n",
    "Use Action to run one of the actions available to you - then return PAUSE.\n",
    "Observation will be the result of running those actions.\n",
    "\n",
    "Your available actions are:\n",
    "\n",
    "calculate:\n",
    "e.g. calculate: 4 * 7 / 3\n",
    "Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\n",
    "\n",
    "get_planet_mass:\n",
    "e.g. get_planet_mass: Earth\n",
    "returns weight of the planet in kg\n",
    "\n",
    "Example session:\n",
    "\n",
    "Question: What is the mass of Earth times 2?\n",
    "Thought: I need to find the mass of Earth\n",
    "Action: get_planet_mass: Earth\n",
    "PAUSE \n",
    "\n",
    "You will be called again with this:\n",
    "\n",
    "Observation: 5.972e24\n",
    "\n",
    "Thought: I need to multiply this by 2\n",
    "Action: calculate: 5.972e24 * 2\n",
    "PAUSE\n",
    "\n",
    "You will be called again with this: \n",
    "\n",
    "Observation: 1,1944×10e25\n",
    "\n",
    "If you have the answer, output it as the Answer.\n",
    "\n",
    "Answer: The mass of Earth times 2 is 1,1944×10e25.\n",
    "\n",
    "Now it's your turn:\n",
    "\"\"\".strip()\n",
    "\n",
    "def calculate(operation: str) -> float:\n",
    "    return eval(operation)\n",
    "\n",
    "\n",
    "def get_planet_mass(planet) -> float:\n",
    "    match planet.lower():\n",
    "        case \"earth\":\n",
    "            return 5.972e24\n",
    "        case \"jupiter\":\n",
    "            return 1.898e27\n",
    "        case \"mars\":\n",
    "            return 6.39e23\n",
    "        case \"mercury\":\n",
    "            return 3.285e23\n",
    "        case \"neptune\":\n",
    "            return 1.024e26\n",
    "        case \"saturn\":\n",
    "            return 5.683e26\n",
    "        case \"uranus\":\n",
    "            return 8.681e25\n",
    "        case \"venus\":\n",
    "            return 4.867e24\n",
    "        case _:\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
